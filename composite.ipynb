{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c556bc-dfbe-41b8-8d4e-91ea975a25e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1.005968\n",
      "Epoch 20, Loss: 0.987744\n",
      "Epoch 30, Loss: 0.968080\n",
      "Epoch 40, Loss: 0.945852\n",
      "Epoch 50, Loss: 0.920684\n",
      "Epoch 60, Loss: 0.892083\n",
      "Epoch 70, Loss: 0.859491\n",
      "Epoch 80, Loss: 0.822325\n",
      "Epoch 90, Loss: 0.781988\n",
      "Epoch 100, Loss: 0.745837\n",
      "Epoch 110, Loss: 0.714583\n",
      "Epoch 120, Loss: 0.686820\n",
      "Epoch 130, Loss: 0.662312\n",
      "Epoch 140, Loss: 0.640435\n",
      "Epoch 150, Loss: 0.620839\n",
      "Epoch 160, Loss: 0.603335\n",
      "Epoch 170, Loss: 0.587862\n",
      "Epoch 180, Loss: 0.574329\n",
      "Epoch 190, Loss: 0.562552\n",
      "Epoch 200, Loss: 0.552280\n",
      "Epoch 210, Loss: 0.543199\n",
      "Epoch 220, Loss: 0.535068\n",
      "Epoch 230, Loss: 0.527708\n",
      "Epoch 240, Loss: 0.520930\n",
      "Epoch 250, Loss: 0.514509\n",
      "Epoch 260, Loss: 0.508184\n",
      "Epoch 270, Loss: 0.501645\n",
      "Epoch 280, Loss: 0.494664\n",
      "Epoch 290, Loss: 0.486989\n",
      "Epoch 300, Loss: 0.478528\n",
      "Epoch 310, Loss: 0.469343\n",
      "Epoch 320, Loss: 0.459785\n",
      "Epoch 330, Loss: 0.450409\n",
      "Epoch 340, Loss: 0.441796\n",
      "Epoch 350, Loss: 0.434070\n",
      "Epoch 360, Loss: 0.426952\n",
      "Epoch 370, Loss: 0.420062\n",
      "Epoch 380, Loss: 0.413168\n",
      "Epoch 390, Loss: 0.406124\n",
      "Epoch 400, Loss: 0.398851\n",
      "Epoch 410, Loss: 0.391352\n",
      "Epoch 420, Loss: 0.383677\n",
      "Epoch 430, Loss: 0.376051\n",
      "Epoch 440, Loss: 0.368649\n",
      "Epoch 450, Loss: 0.361580\n",
      "Epoch 460, Loss: 0.354889\n",
      "Epoch 470, Loss: 0.348621\n",
      "Epoch 480, Loss: 0.342767\n",
      "Epoch 490, Loss: 0.337323\n",
      "Epoch 500, Loss: 0.332263\n",
      "Epoch 510, Loss: 0.327555\n",
      "Epoch 520, Loss: 0.323173\n",
      "Epoch 530, Loss: 0.319082\n",
      "Epoch 540, Loss: 0.315255\n",
      "Epoch 550, Loss: 0.311664\n",
      "Epoch 560, Loss: 0.308287\n",
      "Epoch 570, Loss: 0.305092\n",
      "Epoch 580, Loss: 0.302054\n",
      "Epoch 590, Loss: 0.299150\n",
      "Epoch 600, Loss: 0.296358\n",
      "Epoch 610, Loss: 0.293662\n",
      "Epoch 620, Loss: 0.291044\n",
      "Epoch 630, Loss: 0.288491\n",
      "Epoch 640, Loss: 0.285996\n",
      "Epoch 650, Loss: 0.283558\n",
      "Epoch 660, Loss: 0.281177\n",
      "Epoch 670, Loss: 0.278850\n",
      "Epoch 680, Loss: 0.276576\n",
      "Epoch 690, Loss: 0.274355\n",
      "Epoch 700, Loss: 0.272184\n",
      "Epoch 710, Loss: 0.270059\n",
      "Epoch 720, Loss: 0.267973\n",
      "Epoch 730, Loss: 0.265919\n",
      "Epoch 740, Loss: 0.263893\n",
      "Epoch 750, Loss: 0.261889\n",
      "Epoch 760, Loss: 0.259900\n",
      "Epoch 770, Loss: 0.257919\n",
      "Epoch 780, Loss: 0.255944\n",
      "Epoch 790, Loss: 0.253975\n",
      "Epoch 800, Loss: 0.252011\n",
      "Epoch 810, Loss: 0.250040\n",
      "Epoch 820, Loss: 0.248056\n",
      "Epoch 830, Loss: 0.246049\n",
      "Epoch 840, Loss: 0.244012\n",
      "Epoch 850, Loss: 0.241936\n",
      "Epoch 860, Loss: 0.239816\n",
      "Epoch 870, Loss: 0.237628\n",
      "Epoch 880, Loss: 0.235357\n",
      "Epoch 890, Loss: 0.232999\n",
      "Epoch 900, Loss: 0.230549\n",
      "Epoch 910, Loss: 0.227987\n",
      "Epoch 920, Loss: 0.225308\n",
      "Epoch 930, Loss: 0.222518\n",
      "Epoch 940, Loss: 0.219603\n",
      "Epoch 950, Loss: 0.216548\n",
      "Epoch 960, Loss: 0.213350\n",
      "Epoch 970, Loss: 0.210023\n",
      "Epoch 980, Loss: 0.206585\n",
      "Epoch 990, Loss: 0.203050\n",
      "Epoch 1000, Loss: 0.199472\n",
      "\n",
      "🔍 Testing: tube4.csv\n",
      "Classification Report (Fused Prediction):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6279    0.9694    0.7622     33428\n",
      "           1     0.8602    0.2468    0.3835     25492\n",
      "\n",
      "    accuracy                         0.6568     58920\n",
      "   macro avg     0.7441    0.6081    0.5729     58920\n",
      "weighted avg     0.7284    0.6568    0.5984     58920\n",
      "\n",
      "Confusion Matrix:\n",
      "[[32406  1022]\n",
      " [19201  6291]]\n",
      "\n",
      "🔍 Testing: tube5.csv\n",
      "Classification Report (Fused Prediction):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6068    0.9669    0.7456    183309\n",
      "           1     0.8577    0.2415    0.3768    151433\n",
      "\n",
      "    accuracy                         0.6387    334742\n",
      "   macro avg     0.7322    0.6042    0.5612    334742\n",
      "weighted avg     0.7203    0.6387    0.5788    334742\n",
      "\n",
      "Confusion Matrix:\n",
      "[[177241   6068]\n",
      " [114868  36565]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "features = ['f6','f7','f8','f9','f10','f11','f12','f13','f14','f15','f16']\n",
    "\n",
    "# ========================\n",
    "# Autoencoder\n",
    "# ========================\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(4, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, input_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "train_files = ['tube1.csv', 'tube2.csv', 'tube3.csv']\n",
    "train_dfs = []\n",
    "\n",
    "for file in train_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df['label'] = (df['f7'] * df['f8'] > 0.1).astype(int)\n",
    "    df = df.dropna(subset=features)\n",
    "    df = df[df['label'] == 0]  \n",
    "    train_dfs.append(df)\n",
    "\n",
    "train_df = pd.concat(train_dfs, ignore_index=True)\n",
    "X_train_raw = train_df[features].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "\n",
    "# ========================\n",
    "# Autoencoder\n",
    "# ========================\n",
    "model = Autoencoder(input_dim=X_train_tensor.shape[1])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    output = model(X_train_tensor)\n",
    "    loss = criterion(output, X_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_reconstructed = model(X_train_tensor)\n",
    "    train_mse = torch.mean((X_train_tensor - train_reconstructed)**2, dim=1).numpy()\n",
    "threshold_ae = np.percentile(train_mse, 90)\n",
    "\n",
    "test_files = ['tube4.csv', 'tube5.csv']\n",
    "for file in test_files:\n",
    "    print(f\"\\n🔍 Testing: {file}\")\n",
    "    df = pd.read_csv(file)\n",
    "    df['label'] = (df['f7'] * df['f8'] > 0.1).astype(int)\n",
    "    df = df.dropna(subset=features)\n",
    "    y_true = df['label'].values\n",
    "\n",
    "    X_test_raw = df[features].values\n",
    "    X_test_scaled = scaler.transform(X_test_raw)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "    # 1. Autoencoder \n",
    "    with torch.no_grad():\n",
    "        reconstructed = model(X_test_tensor)\n",
    "        test_mse = torch.mean((X_test_tensor - reconstructed)**2, dim=1).numpy()\n",
    "    ae_pred = (test_mse > threshold_ae).astype(int)\n",
    "\n",
    "    # 2. LOF \n",
    "    lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "    lof_pred = lof.fit_predict(X_test_scaled)  # -1 = outlier\n",
    "    lof_pred = (lof_pred == -1).astype(int)\n",
    "\n",
    "    # 3. Isolation Forest \n",
    "    isof = IsolationForest(contamination=0.1, random_state=42)\n",
    "    isof_pred = isof.fit_predict(X_test_scaled)\n",
    "    isof_pred = (isof_pred == -1).astype(int)\n",
    "\n",
    "    votes = ae_pred + lof_pred + isof_pred\n",
    "    fused_pred = (votes >= 2).astype(int)\n",
    "\n",
    "    print(\"Classification Report (Fused Prediction):\")\n",
    "    print(classification_report(y_true, fused_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, fused_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13dd1d55-007e-43be-a1da-1b27ec6117ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1.032690\n",
      "Epoch 20, Loss: 1.004716\n",
      "Epoch 30, Loss: 0.976374\n",
      "Epoch 40, Loss: 0.946350\n",
      "Epoch 50, Loss: 0.914083\n",
      "Epoch 60, Loss: 0.879768\n",
      "Epoch 70, Loss: 0.844330\n",
      "Epoch 80, Loss: 0.809374\n",
      "Epoch 90, Loss: 0.776697\n",
      "Epoch 100, Loss: 0.747058\n",
      "Epoch 110, Loss: 0.719825\n",
      "Epoch 120, Loss: 0.694025\n",
      "Epoch 130, Loss: 0.669016\n",
      "Epoch 140, Loss: 0.644494\n",
      "Epoch 150, Loss: 0.620905\n",
      "Epoch 160, Loss: 0.599353\n",
      "Epoch 170, Loss: 0.580739\n",
      "Epoch 180, Loss: 0.564962\n",
      "Epoch 190, Loss: 0.551395\n",
      "Epoch 200, Loss: 0.539516\n",
      "Epoch 210, Loss: 0.529003\n",
      "Epoch 220, Loss: 0.519612\n",
      "Epoch 230, Loss: 0.511127\n",
      "Epoch 240, Loss: 0.503389\n",
      "Epoch 250, Loss: 0.496312\n",
      "Epoch 260, Loss: 0.489854\n",
      "Epoch 270, Loss: 0.483967\n",
      "Epoch 280, Loss: 0.478586\n",
      "Epoch 290, Loss: 0.473646\n",
      "Epoch 300, Loss: 0.469082\n",
      "Epoch 310, Loss: 0.464820\n",
      "Epoch 320, Loss: 0.460789\n",
      "Epoch 330, Loss: 0.456905\n",
      "Epoch 340, Loss: 0.453078\n",
      "Epoch 350, Loss: 0.449224\n",
      "Epoch 360, Loss: 0.445226\n",
      "Epoch 370, Loss: 0.440937\n",
      "Epoch 380, Loss: 0.436180\n",
      "Epoch 390, Loss: 0.430771\n",
      "Epoch 400, Loss: 0.424546\n",
      "Epoch 410, Loss: 0.417440\n",
      "Epoch 420, Loss: 0.409244\n",
      "Epoch 430, Loss: 0.399945\n",
      "Epoch 440, Loss: 0.389963\n",
      "Epoch 450, Loss: 0.380035\n",
      "Epoch 460, Loss: 0.370827\n",
      "Epoch 470, Loss: 0.362626\n",
      "Epoch 480, Loss: 0.355441\n",
      "Epoch 490, Loss: 0.349117\n",
      "Epoch 500, Loss: 0.343498\n",
      "Epoch 510, Loss: 0.338477\n",
      "Epoch 520, Loss: 0.334002\n",
      "Epoch 530, Loss: 0.330040\n",
      "Epoch 540, Loss: 0.326546\n",
      "Epoch 550, Loss: 0.323468\n",
      "Epoch 560, Loss: 0.320754\n",
      "Epoch 570, Loss: 0.318350\n",
      "Epoch 580, Loss: 0.316205\n",
      "Epoch 590, Loss: 0.314276\n",
      "Epoch 600, Loss: 0.312522\n",
      "Epoch 610, Loss: 0.310908\n",
      "Epoch 620, Loss: 0.309405\n",
      "Epoch 630, Loss: 0.307985\n",
      "Epoch 640, Loss: 0.306628\n",
      "Epoch 650, Loss: 0.305315\n",
      "Epoch 660, Loss: 0.304034\n",
      "Epoch 670, Loss: 0.302774\n",
      "Epoch 680, Loss: 0.301527\n",
      "Epoch 690, Loss: 0.300287\n",
      "Epoch 700, Loss: 0.299047\n",
      "Epoch 710, Loss: 0.297802\n",
      "Epoch 720, Loss: 0.296543\n",
      "Epoch 730, Loss: 0.295241\n",
      "Epoch 740, Loss: 0.293881\n",
      "Epoch 750, Loss: 0.292461\n",
      "Epoch 760, Loss: 0.290960\n",
      "Epoch 770, Loss: 0.289340\n",
      "Epoch 780, Loss: 0.287556\n",
      "Epoch 790, Loss: 0.285555\n",
      "Epoch 800, Loss: 0.283273\n",
      "\n",
      "🔍 Testing: tube4.csv\n",
      "Classification Report (Fused Prediction):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9378    0.9783    0.9576     54415\n",
      "           1     0.4513    0.2158    0.2919      4505\n",
      "\n",
      "    accuracy                         0.9200     58920\n",
      "   macro avg     0.6945    0.5970    0.6248     58920\n",
      "weighted avg     0.9006    0.9200    0.9067     58920\n",
      "\n",
      "Confusion Matrix:\n",
      "[[53233  1182]\n",
      " [ 3533   972]]\n",
      "\n",
      "🔍 Testing: tube5.csv\n",
      "Classification Report (Fused Prediction):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9293    0.9752    0.9517    305169\n",
      "           1     0.4791    0.2349    0.3152     29573\n",
      "\n",
      "    accuracy                         0.9098    334742\n",
      "   macro avg     0.7042    0.6051    0.6335    334742\n",
      "weighted avg     0.8896    0.9098    0.8955    334742\n",
      "\n",
      "Confusion Matrix:\n",
      "[[297616   7553]\n",
      " [ 22627   6946]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "features = ['f6','f7','f8','f9','f10','f11','f12','f13','f14','f15','f16']\n",
    "\n",
    "# ========================\n",
    "# Autoencoder\n",
    "# ========================\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(4, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, input_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "train_files = ['tube1.csv', 'tube2.csv', 'tube3.csv']\n",
    "train_dfs = []\n",
    "\n",
    "for file in train_files:\n",
    "    df = pd.read_csv(file)\n",
    "    f7_rounded = df['f7'].round(6)\n",
    "    df['label'] = (~f7_rounded.isin([0.857143, 0.714286])).astype(int)\n",
    "    df = df.dropna(subset=features)\n",
    "    df = df[df['label'] == 0]\n",
    "    train_dfs.append(df)\n",
    "\n",
    "train_df = pd.concat(train_dfs, ignore_index=True)\n",
    "X_train_raw = train_df[features].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "\n",
    "# ========================\n",
    "# Autoencoder\n",
    "# ========================\n",
    "model = Autoencoder(input_dim=X_train_tensor.shape[1])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(800):\n",
    "    output = model(X_train_tensor)\n",
    "    loss = criterion(output, X_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_reconstructed = model(X_train_tensor)\n",
    "    train_mse = torch.mean((X_train_tensor - train_reconstructed)**2, dim=1).numpy()\n",
    "threshold_ae = np.percentile(train_mse, 90)\n",
    "\n",
    "\n",
    "test_files = ['tube4.csv', 'tube5.csv']\n",
    "for file in test_files:\n",
    "    print(f\"\\n🔍 Testing: {file}\")\n",
    "    df = pd.read_csv(file)\n",
    "    f7_rounded = df['f7'].round(6)\n",
    "    df['label'] = (~f7_rounded.isin([0.857143, 0.714286])).astype(int)\n",
    "    df = df.dropna(subset=features)\n",
    "    y_true = df['label'].values\n",
    "\n",
    "    X_test_raw = df[features].values\n",
    "    X_test_scaled = scaler.transform(X_test_raw)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "    # 1. Autoencoder \n",
    "    with torch.no_grad():\n",
    "        reconstructed = model(X_test_tensor)\n",
    "        test_mse = torch.mean((X_test_tensor - reconstructed)**2, dim=1).numpy()\n",
    "    ae_pred = (test_mse > threshold_ae).astype(int)\n",
    "\n",
    "    # 2. LOF \n",
    "    lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "    lof_pred = lof.fit_predict(X_test_scaled)  # -1 = outlier\n",
    "    lof_pred = (lof_pred == -1).astype(int)\n",
    "\n",
    "    # 3. Isolation Forest \n",
    "    isof = IsolationForest(contamination=0.05, random_state=42)\n",
    "    isof_pred = isof.fit_predict(X_test_scaled)\n",
    "    isof_pred = (isof_pred == -1).astype(int)\n",
    "\n",
    "    # Majority Voting\n",
    "    votes = ae_pred + lof_pred + isof_pred\n",
    "    fused_pred = (votes >= 2).astype(int)\n",
    "\n",
    "    # --- 评估 ---\n",
    "    print(\"Classification Report (Fused Prediction):\")\n",
    "    print(classification_report(y_true, fused_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, fused_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6cc11b-447b-4178-b5db-d33fc4e8a850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
